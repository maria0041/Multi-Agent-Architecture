{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1BMGFAFd2y"
      },
      "source": [
        "# **Multi Agent Architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arzd78dVHOpe"
      },
      "source": [
        "Run the cell below by clicking on the ▶ play button on the left of the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkftcNfcFVR7",
        "outputId": "530a56d3-a24c-4e8d-b677-21fed9c23727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.45.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Downloading anthropic-0.45.0-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.45.0\n"
          ]
        }
      ],
      "source": [
        "%pip install anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcnQRE1mHxAe"
      },
      "source": [
        "Enter your API key in the `api_key` variable field in the cell below. If you do not want a system prompt, simply delete that whole parameter.\n",
        "\n",
        "Then click on the ▶ play button on the left of the cell to run the prompt.Claude's response will appear below the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "q7qQmv5UICt1",
        "outputId": "9bd56c8d-6230-4efe-d47a-a28c2c1956f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Prompt: 3+5\n",
            "Iteration 1:\n"
          ]
        },
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-79eb0711dfbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0muser_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the Prompt: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFinal Solution:\\n{solution}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-79eb0711dfbf>\u001b[0m in \u001b[0;36msolve_problem\u001b[0;34m(user_prompt, delay, max_iterations)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Step 1: Problem Definer Agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mproblem_definer_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconstruct_agent_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Problem Definer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevised_problem_statement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mproblem_definition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproblem_definer_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproblem_definer_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Problem Definer Output:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem_definition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    902\u001b[0m             )\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         )\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
          ]
        }
      ],
      "source": [
        "import anthropic\n",
        "import re\n",
        "import time\n",
        "\n",
        "client = anthropic.Anthropic(api_key=\" \")\n",
        "MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "MAX_TOKEN = 800\n",
        "\n",
        "def extract_between_tags(tag, string, strip=False):\n",
        "    ext_list = re.findall(fr\"<{tag}>(.+?)</{tag}>\", string, re.DOTALL)\n",
        "    if strip:\n",
        "        ext_list = [e.strip() for e in ext_list]\n",
        "    return ext_list\n",
        "\n",
        "def construct_system_prompt(agent_name, agent_prompt):\n",
        "    return f\"System Prompt: {agent_prompt}\\nYou are the {agent_name} Agent.\"\n",
        "\n",
        "def construct_agent_prompt(agent_name, agent_input):\n",
        "    return f\"You are the {agent_name} Agent. {agent_input}\"\n",
        "\n",
        "problem_definer_prompt = construct_system_prompt(\"Problem Definer\", \"Your role is to understand the problem statement provided by the user and clarify any ambiguities through clarifying questions. think in steps, provide only what is necessary\")\n",
        "decomposer_prompt = construct_system_prompt(\"Decomposer\", \"Your task is to break down the problem into specific subtasks related to the problem statement provided by the Problem Definer Agent.provide only what is necessary\")\n",
        "generator_prompt = construct_system_prompt(\"Generator\", \"Based on the subtasks provided by the Decomposer Agent, generate prompts for the Worker Agents to write Python code or provide solutions for those specific subtasks.provide only what is necessary\")\n",
        "worker_prompt = construct_system_prompt(\"Worker\", \"You are a Worker Agent tasked with writing Python code or providing a solution for the specific subtask assigned to you by the Generator Agent. Follow the prompt provided and do not deviate from the assigned task. EXAMPLE: <worker_prompt>Prompt for the Worker Agent:</worker_prompt><worker_prompt>Given the following system of linear equations:\\n\\n2x + 5y = 0\\n3x - 2y = 0\\n\\nWrite a Python function to solve this system of linear equations and find the value(s) of x and y. The function should return a tuple containing the values of x and y, or None if no solution exists.\\n\\nYou can use any appropriate method to solve the system, such as Gaussian elimination, substitution, or elimination.\\n\\nHere's an example of how the function should be structured:\\n\\ndef solve_linear_system():\\n    # Your code to solve the system of linear equations\\n    # ...\\n\\n    # If a solution exists, return a tuple (x, y)\\n    # Otherwise, return None\\n\\n    return (x, y)\\n</worker_prompt>, provide only what is necessary\")\n",
        "compiler_prompt = construct_system_prompt(\"Compiler\", \"Your role is to combine the solutions provided by the Worker Agents into a unified final solution without introducing any new content or solutions.provide only what is necessary\")\n",
        "tester_prompt = construct_system_prompt(\"Tester\", \"Test the final solution provided by the Compiler Agent against the original problem statement and provide clear feedback on its correctness and completeness.provide only what is necessary\")\n",
        "error_identifier_prompt = construct_system_prompt(\"Error Identifier\", \"Based on the feedback from the Tester Agent, identify and categorize any missing components, errors, or flaws in the final solution in detail.provide only what is necessary\")\n",
        "editor_prompt = construct_system_prompt(\"Editor\", \"Based on the detailed feedback from the Error Identifier, suggest specific improvements or additions to the workflow or the final solution to address the identified issues.provide only what is necessary\")\n",
        "\n",
        "def solve_problem(user_prompt, delay=2, max_iterations=10):\n",
        "    problem_solved = False\n",
        "    iteration = 1\n",
        "    revised_problem_statement = user_prompt\n",
        "    conversation_history = \"\"\n",
        "\n",
        "    while not problem_solved and iteration <= max_iterations:\n",
        "        print(f\"Iteration {iteration}:\")\n",
        "\n",
        "        # Step 1: Problem Definer Agent\n",
        "        problem_definer_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Problem Definer\", revised_problem_statement)}]\n",
        "        problem_definition = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=problem_definer_messages, system=problem_definer_prompt).content[0].text\n",
        "        print(\"Problem Definer Output:\")\n",
        "        print(problem_definition)\n",
        "        time.sleep(delay)\n",
        "        revised_problem_statement = problem_definition\n",
        "        conversation_history += \"\\n\" + problem_definition\n",
        "\n",
        "        # Step 2: Decomposer Agent\n",
        "        decomposer_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Decomposer\", f\"Based on the problem definition provided by the Problem Definer Agent, decompose the problem into smaller subtasks.\\n\\n{conversation_history}\")}]\n",
        "        subtasks = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=decomposer_messages, system=decomposer_prompt).content[0].text\n",
        "        print(\"\\nDecomposer Output:\")\n",
        "        print(subtasks)\n",
        "        time.sleep(delay)\n",
        "        conversation_history += \"\\n\" + subtasks\n",
        "\n",
        "        # Step 3: Generator Agent\n",
        "        generator_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Generator\", f\"Based on the subtasks provided by the Decomposer Agent, generate prompts for the Worker Agents to write Python code or provide solutions for those specific subtasks.\\n\\n{conversation_history}\")}]\n",
        "        worker_prompts = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=generator_messages, system=generator_prompt).content[0].text\n",
        "        print(\"\\nGenerator Output:\")\n",
        "        print(worker_prompts)\n",
        "        time.sleep(delay)\n",
        "        conversation_history += \"\\n\" + worker_prompts\n",
        "\n",
        "        # Step 4: Worker Agents\n",
        "        worker_solutions = []\n",
        "        for worker_prompt in extract_between_tags(\"worker_prompt\", worker_prompts):\n",
        "            worker_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Worker\", worker_prompt, conversation_history)}]\n",
        "            worker_solution = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=worker_messages, system=worker_prompt).content[0].text\n",
        "            print(f\"\\nWorker Agent Output for Task '{worker_prompt}':\")\n",
        "            print(worker_solution)\n",
        "\n",
        "            # Check if the Worker Agent provided a valid solution\n",
        "            if \"Unable to provide a solution\" in worker_solution or worker_solution.strip() == \"\":\n",
        "                print(\"Worker Agent unable to provide a solution for this task. Skipping to the next task.\")\n",
        "                continue\n",
        "            elif \"Clarification needed\" in worker_solution:\n",
        "                # Implement a mechanism for the Worker Agent to request clarification\n",
        "                # and collaborate with other agents to resolve the issue\n",
        "                pass\n",
        "\n",
        "            worker_solutions.append(worker_solution)\n",
        "            conversation_history += f\"\\nWorker Agent Output for Task '{worker_prompt}':\\n{worker_solution}\"\n",
        "            time.sleep(delay)\n",
        "\n",
        "        # Step 5: Compiler Agent\n",
        "        compiler_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Compiler\", f\"Combine the solutions provided by the Worker Agents into a unified final solution without introducing any new content or solutions. If any Worker Agent did not provide a valid solution, skip that subtask and move on to the next.\\n\\n{''.join(worker_solutions)}\")}]\n",
        "        final_solution = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=compiler_messages, system=compiler_prompt).content[0].text\n",
        "        print(\"\\nCompiler Output:\")\n",
        "        print(final_solution)\n",
        "        time.sleep(delay)\n",
        "        conversation_history += \"\\n\" + final_solution\n",
        "\n",
        "        # Step 6: Tester Agent\n",
        "        tester_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Tester\", f\"Test the final solution provided by the Compiler Agent against the original problem statement. Provide detailed feedback on the correctness and completeness of the solution, including any missing or incorrect components.\\n\\n{conversation_history}\")}]\n",
        "        test_result = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=tester_messages, system=tester_prompt).content[0].text\n",
        "        print(\"\\nTester Output:\")\n",
        "        print(test_result)\n",
        "        time.sleep(delay)\n",
        "        conversation_history += \"\\n\" + test_result\n",
        "\n",
        "        # Calculate the solution quality score\n",
        "        solution_quality_score = evaluate_solution_quality(test_result)\n",
        "        print(f\"Solution Quality Score: {solution_quality_score}\")\n",
        "\n",
        "        # Step 7: Error Identifier\n",
        "        error_identifier_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Error Identifier\", f\"Based on the feedback from the Tester Agent, identify and categorize any missing components, errors, or flaws in the final solution in detail.\\n\\n{conversation_history}\")}]\n",
        "        identified_errors = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=error_identifier_messages, system=error_identifier_prompt).content[0].text\n",
        "\n",
        "        print(\"\\nError Identifier Output:\")\n",
        "        print(identified_errors)\n",
        "        time.sleep(delay)\n",
        "        conversation_history += \"\\n\" + identified_errors\n",
        "\n",
        "        # Step 8: Editor Agent\n",
        "        editor_messages = [{\"role\": \"user\", \"content\": construct_agent_prompt(\"Editor\", f\"Based on the detailed feedback from the Error Identifier, suggest specific improvements or additions to the workflow or the final solution to address the identified issues.\\n\\n{conversation_history}\")}]\n",
        "        suggested_edits = client.messages.create(model=MODEL_NAME, max_tokens=MAX_TOKEN, messages=editor_messages, system=editor_prompt).content[0].text\n",
        "        print(\"\\nEditor Output:\")\n",
        "        print(suggested_edits)\n",
        "        time.sleep(delay)\n",
        "        conversation_history += \"\\n\" + suggested_edits\n",
        "\n",
        "        if solution_quality_score >= 0.9:\n",
        "            problem_solved = True\n",
        "            print(\"\\nProblem solved successfully!\")\n",
        "        else:\n",
        "            revised_problem_statement = suggested_edits\n",
        "            iteration += 1\n",
        "\n",
        "    return final_solution\n",
        "\n",
        "def evaluate_solution_quality(test_result):\n",
        "    if \"no issues\" in test_result.lower() or \"correct\" in test_result.lower():\n",
        "        return 1.0\n",
        "    elif \"minor issues\" in test_result.lower():\n",
        "        return 0.7\n",
        "    elif \"significant issues\" in test_result.lower():\n",
        "        return 0.4\n",
        "    else:\n",
        "        return 0.1\n",
        "\n",
        "user_prompt = input(\"Enter the Prompt: \")\n",
        "solution = solve_problem(user_prompt, delay=5, max_iterations=3)\n",
        "print(f\"\\nFinal Solution:\\n{solution}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
